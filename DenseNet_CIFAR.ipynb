{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DenseNet - cifar10.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wVIx_KIigxPV",
        "outputId": "39aad4d8-76cb-40ac-fa41-940d5a7fb545",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# import keras\n",
        "# from keras.datasets import cifar10\n",
        "# from keras.models import Model, Sequential\n",
        "# from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "# from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "# from keras.layers import Concatenate\n",
        "# from keras.optimizers import Adam\n",
        "\n",
        "!pip install -q keras\n",
        "\n",
        "import keras\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, Flatten\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRyWCd_zMvlQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install tensorflow-gpu==1.5.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfCgnl7jMGpk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import tensorflow as tf\n",
        "#tf.__version__\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UNHw6luQg3gc",
        "colab": {}
      },
      "source": [
        "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
        "# backend\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "from keras import backend as k\n",
        "\n",
        "# Don't pre-allocate memory; allocate as-needed\n",
        "# import tensorflow as tf\n",
        "\n",
        "#tf.config.gpu.set_per_process_memory_fraction(0.75)\n",
        "#tf.config.gpu.set_per_process_memory_growth(True)\n",
        "\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "# Create a session with the above options specified.\n",
        "# k.tensorflow_backend.set_session(tf.Session(config=config))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dsO_yGxcg5D8",
        "colab": {}
      },
      "source": [
        "# Hyperparameters\n",
        "\n",
        "\n",
        "#batch_size = 128\n",
        "batch_size = 64\n",
        "num_classes = 10\n",
        "\n",
        "epochs = 35\n",
        "\n",
        "l = 6\n",
        "\n",
        "num_filter = 35\n",
        "\n",
        "compression = 1\n",
        "dropout_rate = 0.3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mB7o3zu1g6eT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d68af6d2-5bb9-450e-cf3c-31dfa8777bbf"
      },
      "source": [
        "# Load CIFAR10 Data\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "img_height, img_width, channel = X_train.shape[1],X_train.shape[2],X_train.shape[3]\n",
        "\n",
        "# convert to one hot encoing \n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes) "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 11s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KzSaZbjZadj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def preprocess_data(data_set):\n",
        "    mean = np.array([125.3, 123.0, 113.9])\n",
        "    std = np.array([63.0, 62.1, 66.7])\n",
        "\n",
        "    data_set -= mean\n",
        "    data_set /= std\n",
        "    return data_set\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train = preprocess_data(X_train)\n",
        "X_test = preprocess_data(X_test)\n",
        "\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPB6Er2QnovM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1iJL16dQXsk",
        "colab_type": "text"
      },
      "source": [
        "# Using IMage Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-Dr_-ptoHhs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#datagen = ImageDataGenerator(\n",
        "#    featurewise_center=True,\n",
        "#    featurewise_std_normalization=True,\n",
        "#    rotation_range=20,\n",
        "#    width_shift_range=0.1,\n",
        "#    height_shift_range=0.1,\n",
        "#    horizontal_flip=True)\n",
        "\n",
        "#datagen.fit(X_train)\n",
        "\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.125,\n",
        "    height_shift_range=0.125,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    zoom_range=0.10\n",
        ")\n",
        "\n",
        "datagen.fit(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGqRIBhPJDq6",
        "colab_type": "code",
        "outputId": "419eab79-0793-4d22-d96e-ce64d1208362",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(45000, 32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kFH5Er4JDre",
        "colab_type": "code",
        "outputId": "0785ce85-61bf-4483-c2e3-c1af070049ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ee-sge5Kg7vr",
        "colab": {}
      },
      "source": [
        "# Dense Block\n",
        "def denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    temp = input\n",
        "    for _ in range(l): \n",
        "        BatchNorm = layers.BatchNormalization()(temp)\n",
        "        relu = layers.Activation('relu')(BatchNorm)\n",
        "        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
        "        if dropout_rate>0:\n",
        "            Conv2D_3_3 = layers.Dropout(dropout_rate)(Conv2D_3_3)\n",
        "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
        "        \n",
        "        temp = concat\n",
        "        \n",
        "    return temp\n",
        "\n",
        "## transition Blosck\n",
        "def transition(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    BatchNorm = layers.BatchNormalization()(input)\n",
        "    relu = layers.Activation('relu')(BatchNorm)\n",
        "    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
        "    if dropout_rate>0:\n",
        "         Conv2D_BottleNeck = layers.Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
        "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
        "    return avg\n",
        "\n",
        "#output layer\n",
        "def output_layer(input):\n",
        "    global compression\n",
        "    BatchNorm = layers.BatchNormalization()(input)\n",
        "    relu = layers.Activation('relu')(BatchNorm)\n",
        "    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)\n",
        "    flat = layers.Flatten()(AvgPooling)\n",
        "    output = layers.Dense(num_classes, activation='softmax')(flat)\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iqgS9isshBp",
        "colab_type": "text"
      },
      "source": [
        "# Chaging the architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "anPCpQWhhGb7",
        "outputId": "26b64bab-23ad-493c-df78-2884c0b129f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "#num_filter = 12\n",
        "#dropout_rate = 0.2\n",
        "\n",
        "#l = 12\n",
        "\n",
        "input = layers.Input(shape=(img_height, img_width, channel,))\n",
        "First_Conv2D = layers.Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
        "\n",
        "First_Block = denseblock(First_Conv2D, num_filter, dropout_rate)\n",
        "First_Transition = transition(First_Block, num_filter, dropout_rate)\n",
        "\n",
        "Second_Block = denseblock(First_Transition, num_filter, dropout_rate)\n",
        "Second_Transition = transition(Second_Block, num_filter, dropout_rate)\n",
        "\n",
        "Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n",
        "Third_Transition = transition(Third_Block, num_filter, dropout_rate)\n",
        "\n",
        "\n",
        "#Fourth_Block = denseblock(Third_Transition, num_filter, dropout_rate)\n",
        "#Fourth_Transition = transition(Fourth_Block, num_filter, dropout_rate)\n",
        "\n",
        "Last_Block = denseblock(Third_Transition,  num_filter, dropout_rate)\n",
        "\n",
        "\n",
        "output = output_layer(Last_Block)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0628 04:38:55.109632 140063208904576 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMDVVsAgJDsL",
        "colab_type": "code",
        "outputId": "707e1a91-141c-4556-d1d6-aabaa7609d3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "source": [
        "#https://arxiv.org/pdf/1608.06993.pdf\n",
        "from IPython.display import IFrame, YouTubeVideo\n",
        "YouTubeVideo(id='-W6y8xnd--U', width=600)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"600\"\n",
              "            height=\"300\"\n",
              "            src=\"https://www.youtube.com/embed/-W6y8xnd--U\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x7f78835fb828>"
            ],
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkz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\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAH//2Q==\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1kFh7pdxhNtT",
        "outputId": "16033224-b185-4e52-8d2f-5d7169adb88b",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 35)   945         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 35)   140         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 35)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 35)   11025       activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 32, 32, 35)   0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 32, 32, 70)   0           conv2d[0][0]                     \n",
            "                                                                 dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 70)   280         concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 70)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 35)   22050       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 32, 32, 35)   0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 105)  0           concatenate[0][0]                \n",
            "                                                                 dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 105)  420         concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 105)  0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 35)   33075       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 32, 32, 35)   0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 32, 32, 140)  0           concatenate_1[0][0]              \n",
            "                                                                 dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 140)  560         concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 140)  0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 35)   44100       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 32, 32, 35)   0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 32, 32, 175)  0           concatenate_2[0][0]              \n",
            "                                                                 dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 175)  700         concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 175)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 35)   55125       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 32, 32, 35)   0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 32, 32, 210)  0           concatenate_3[0][0]              \n",
            "                                                                 dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 210)  840         concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 210)  0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 35)   66150       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 32, 32, 35)   0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 32, 32, 245)  0           concatenate_4[0][0]              \n",
            "                                                                 dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 245)  980         concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 245)  0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 35)   8575        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 32, 32, 35)   0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 16, 16, 35)   0           dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 16, 16, 35)   140         average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 16, 16, 35)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 16, 16, 35)   11025       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 16, 16, 35)   0           conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 16, 16, 70)   0           average_pooling2d[0][0]          \n",
            "                                                                 dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 16, 16, 70)   280         concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 16, 16, 70)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 16, 16, 35)   22050       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 16, 16, 35)   0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 16, 16, 105)  0           concatenate_6[0][0]              \n",
            "                                                                 dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 16, 16, 105)  420         concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 16, 16, 105)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 35)   33075       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 16, 16, 35)   0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 16, 16, 140)  0           concatenate_7[0][0]              \n",
            "                                                                 dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 16, 140)  560         concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 16, 16, 140)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 35)   44100       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 16, 16, 35)   0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 16, 16, 175)  0           concatenate_8[0][0]              \n",
            "                                                                 dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 175)  700         concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 175)  0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 35)   55125       activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 16, 16, 35)   0           conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 16, 16, 210)  0           concatenate_9[0][0]              \n",
            "                                                                 dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 210)  840         concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 210)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 16, 16, 35)   66150       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 16, 16, 35)   0           conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 16, 16, 245)  0           concatenate_10[0][0]             \n",
            "                                                                 dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 16, 16, 245)  980         concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 245)  0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 16, 16, 35)   8575        activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 16, 16, 35)   0           conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 8, 8, 35)     0           dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 8, 8, 35)     140         average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 8, 8, 35)     0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 8, 8, 35)     11025       activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 8, 8, 35)     0           conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 8, 8, 70)     0           average_pooling2d_1[0][0]        \n",
            "                                                                 dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 8, 8, 70)     280         concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 8, 8, 70)     0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 8, 8, 35)     22050       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 8, 8, 35)     0           conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 8, 8, 105)    0           concatenate_12[0][0]             \n",
            "                                                                 dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 8, 8, 105)    420         concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 8, 8, 105)    0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 8, 8, 35)     33075       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 8, 8, 35)     0           conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 8, 8, 140)    0           concatenate_13[0][0]             \n",
            "                                                                 dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 8, 8, 140)    560         concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 8, 8, 140)    0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 8, 8, 35)     44100       activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 8, 8, 35)     0           conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_15 (Concatenate)    (None, 8, 8, 175)    0           concatenate_14[0][0]             \n",
            "                                                                 dropout_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 8, 8, 175)    700         concatenate_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 8, 8, 175)    0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 8, 8, 35)     55125       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_18 (Dropout)            (None, 8, 8, 35)     0           conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 8, 8, 210)    0           concatenate_15[0][0]             \n",
            "                                                                 dropout_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 8, 8, 210)    840         concatenate_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 8, 8, 210)    0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 8, 8, 35)     66150       activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 8, 8, 35)     0           conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 8, 8, 245)    0           concatenate_16[0][0]             \n",
            "                                                                 dropout_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 8, 8, 245)    980         concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 8, 8, 245)    0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 8, 8, 35)     8575        activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_20 (Dropout)            (None, 8, 8, 35)     0           conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 4, 4, 35)     0           dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 4, 4, 35)     140         average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 4, 4, 35)     0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 4, 4, 35)     11025       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_21 (Dropout)            (None, 4, 4, 35)     0           conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_18 (Concatenate)    (None, 4, 4, 70)     0           average_pooling2d_2[0][0]        \n",
            "                                                                 dropout_21[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 4, 4, 70)     280         concatenate_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 4, 4, 70)     0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 4, 4, 35)     22050       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_22 (Dropout)            (None, 4, 4, 35)     0           conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_19 (Concatenate)    (None, 4, 4, 105)    0           concatenate_18[0][0]             \n",
            "                                                                 dropout_22[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 4, 4, 105)    420         concatenate_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 4, 4, 105)    0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 4, 4, 35)     33075       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_23 (Dropout)            (None, 4, 4, 35)     0           conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_20 (Concatenate)    (None, 4, 4, 140)    0           concatenate_19[0][0]             \n",
            "                                                                 dropout_23[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 4, 4, 140)    560         concatenate_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 4, 4, 140)    0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 4, 4, 35)     44100       activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (None, 4, 4, 35)     0           conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_21 (Concatenate)    (None, 4, 4, 175)    0           concatenate_20[0][0]             \n",
            "                                                                 dropout_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 4, 4, 175)    700         concatenate_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 4, 4, 175)    0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 4, 4, 35)     55125       activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_25 (Dropout)            (None, 4, 4, 35)     0           conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_22 (Concatenate)    (None, 4, 4, 210)    0           concatenate_21[0][0]             \n",
            "                                                                 dropout_25[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 4, 4, 210)    840         concatenate_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 4, 4, 210)    0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 4, 4, 35)     66150       activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_26 (Dropout)            (None, 4, 4, 35)     0           conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_23 (Concatenate)    (None, 4, 4, 245)    0           concatenate_22[0][0]             \n",
            "                                                                 dropout_26[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 4, 4, 245)    980         concatenate_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 4, 4, 245)    0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 2, 2, 245)    0           activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 980)          0           average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           9810        flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 978,260\n",
            "Trainable params: 970,420\n",
            "Non-trainable params: 7,840\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b4XOsW3ahSkL",
        "colab": {}
      },
      "source": [
        "# determine Loss function and Optimizer\n",
        "\n",
        "\n",
        "#rms = RMSprop(lr=0.001,decay=1e-6)\n",
        "\n",
        "#aam_opt = Adam(lr=0.1,decay=1e-6)\n",
        "\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHb9EO8pyA8Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####  https://www.kaggle.com/genesis16/densenet-93-accuracy\n",
        "\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping, LearningRateScheduler, CSVLogger\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 5, min_lr = 0.000001)\n",
        "\n",
        "early_stop = EarlyStopping(monitor = \"val_loss\", patience = 10)\n",
        "\n",
        "def decay_fn(epoch, lr):\n",
        "    if epoch < 50:\n",
        "        return 0.001\n",
        "    elif epoch >= 50 and epoch < 75:\n",
        "        return 0.0001\n",
        "    else:\n",
        "        return 0.00001\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(decay_fn)\n",
        "\n",
        "csv_logger = CSVLogger('training.log')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "crhGk7kEhXAz",
        "outputId": "210f0e6e-5712-4786-bb00-3f620a8bb3e6",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#model.fit(X_train, y_train,\n",
        "#                    batch_size=batch_size,\n",
        "#                    epochs=epochs,\n",
        "#                    verbose=1, \n",
        "#                    validation_data=(X_test, y_test))\n",
        "\n",
        "history = model.fit_generator(\n",
        "    datagen.flow(X_train, y_train, batch_size=32),\n",
        "    steps_per_epoch=(len(X_train)/batch_size)*5,\n",
        "    epochs=epochs,\n",
        "    verbose = 1,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks = [lr_scheduler, csv_logger]\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/35\n",
            "3516/3515 [==============================] - 542s 154ms/step - loss: 1.3503 - acc: 0.5097 - val_loss: 1.8543 - val_acc: 0.5222\n",
            "Epoch 2/35\n",
            "3516/3515 [==============================] - 528s 150ms/step - loss: 0.9411 - acc: 0.6689 - val_loss: 1.0057 - val_acc: 0.6866\n",
            "Epoch 3/35\n",
            "3516/3515 [==============================] - 526s 149ms/step - loss: 0.7894 - acc: 0.7240 - val_loss: 1.1683 - val_acc: 0.6772\n",
            "Epoch 4/35\n",
            "3516/3515 [==============================] - 526s 149ms/step - loss: 0.6992 - acc: 0.7554 - val_loss: 0.9296 - val_acc: 0.7264\n",
            "Epoch 5/35\n",
            "3516/3515 [==============================] - 525s 149ms/step - loss: 0.6359 - acc: 0.7796 - val_loss: 0.7024 - val_acc: 0.7904\n",
            "Epoch 6/35\n",
            "3516/3515 [==============================] - 525s 149ms/step - loss: 0.5880 - acc: 0.7963 - val_loss: 0.6702 - val_acc: 0.8052\n",
            "Epoch 7/35\n",
            "3516/3515 [==============================] - 525s 149ms/step - loss: 0.5505 - acc: 0.8084 - val_loss: 0.5472 - val_acc: 0.8254\n",
            "Epoch 8/35\n",
            "3516/3515 [==============================] - 525s 149ms/step - loss: 0.5220 - acc: 0.8190 - val_loss: 0.6322 - val_acc: 0.8062\n",
            "Epoch 9/35\n",
            "3516/3515 [==============================] - 525s 149ms/step - loss: 0.4961 - acc: 0.8269 - val_loss: 0.4843 - val_acc: 0.8510\n",
            "Epoch 10/35\n",
            "3516/3515 [==============================] - 525s 149ms/step - loss: 0.4769 - acc: 0.8335 - val_loss: 0.5008 - val_acc: 0.8434\n",
            "Epoch 11/35\n",
            "3516/3515 [==============================] - 525s 149ms/step - loss: 0.4579 - acc: 0.8396 - val_loss: 0.4929 - val_acc: 0.8452\n",
            "Epoch 12/35\n",
            "3516/3515 [==============================] - 525s 149ms/step - loss: 0.4383 - acc: 0.8481 - val_loss: 0.4425 - val_acc: 0.8552\n",
            "Epoch 13/35\n",
            "3516/3515 [==============================] - 525s 149ms/step - loss: 0.4251 - acc: 0.8531 - val_loss: 0.4627 - val_acc: 0.8656\n",
            "Epoch 14/35\n",
            "3516/3515 [==============================] - 526s 150ms/step - loss: 0.4115 - acc: 0.8572 - val_loss: 0.4700 - val_acc: 0.8564\n",
            "Epoch 15/35\n",
            "3516/3515 [==============================] - 524s 149ms/step - loss: 0.3971 - acc: 0.8616 - val_loss: 0.5228 - val_acc: 0.8492\n",
            "Epoch 16/35\n",
            "3516/3515 [==============================] - 524s 149ms/step - loss: 0.3931 - acc: 0.8623 - val_loss: 0.3808 - val_acc: 0.8860\n",
            "Epoch 17/35\n",
            "3516/3515 [==============================] - 527s 150ms/step - loss: 0.3807 - acc: 0.8681 - val_loss: 0.4462 - val_acc: 0.8672\n",
            "Epoch 18/35\n",
            "3516/3515 [==============================] - 526s 150ms/step - loss: 0.3702 - acc: 0.8719 - val_loss: 0.4304 - val_acc: 0.8742\n",
            "Epoch 19/35\n",
            "3516/3515 [==============================] - 526s 150ms/step - loss: 0.3645 - acc: 0.8736 - val_loss: 0.3858 - val_acc: 0.8862\n",
            "Epoch 20/35\n",
            "3516/3515 [==============================] - 525s 149ms/step - loss: 0.3507 - acc: 0.8778 - val_loss: 0.3725 - val_acc: 0.8848\n",
            "Epoch 21/35\n",
            "3516/3515 [==============================] - 526s 150ms/step - loss: 0.3450 - acc: 0.8795 - val_loss: 0.3689 - val_acc: 0.8904\n",
            "Epoch 22/35\n",
            "3516/3515 [==============================] - 527s 150ms/step - loss: 0.3385 - acc: 0.8817 - val_loss: 0.3493 - val_acc: 0.8910\n",
            "Epoch 23/35\n",
            "3516/3515 [==============================] - 526s 150ms/step - loss: 0.3349 - acc: 0.8829 - val_loss: 0.4309 - val_acc: 0.8706\n",
            "Epoch 24/35\n",
            "3516/3515 [==============================] - 525s 149ms/step - loss: 0.3294 - acc: 0.8852 - val_loss: 0.3484 - val_acc: 0.8910\n",
            "Epoch 25/35\n",
            "3516/3515 [==============================] - 526s 150ms/step - loss: 0.3226 - acc: 0.8884 - val_loss: 0.3459 - val_acc: 0.8974\n",
            "Epoch 26/35\n",
            "3516/3515 [==============================] - 525s 149ms/step - loss: 0.3173 - acc: 0.8895 - val_loss: 0.3900 - val_acc: 0.8812\n",
            "Epoch 27/35\n",
            "3516/3515 [==============================] - 524s 149ms/step - loss: 0.3120 - acc: 0.8909 - val_loss: 0.3797 - val_acc: 0.8862\n",
            "Epoch 28/35\n",
            "3516/3515 [==============================] - 525s 149ms/step - loss: 0.3062 - acc: 0.8933 - val_loss: 0.3315 - val_acc: 0.8954\n",
            "Epoch 29/35\n",
            "3516/3515 [==============================] - 526s 150ms/step - loss: 0.3020 - acc: 0.8948 - val_loss: 0.3464 - val_acc: 0.8966\n",
            "Epoch 30/35\n",
            "3516/3515 [==============================] - 529s 150ms/step - loss: 0.2990 - acc: 0.8953 - val_loss: 0.3264 - val_acc: 0.8974\n",
            "Epoch 31/35\n",
            "3516/3515 [==============================] - 531s 151ms/step - loss: 0.2925 - acc: 0.8981 - val_loss: 0.3289 - val_acc: 0.8928\n",
            "Epoch 32/35\n",
            "3516/3515 [==============================] - 534s 152ms/step - loss: 0.2885 - acc: 0.8986 - val_loss: 0.3454 - val_acc: 0.8980\n",
            "Epoch 33/35\n",
            "3516/3515 [==============================] - 535s 152ms/step - loss: 0.2847 - acc: 0.9006 - val_loss: 0.3249 - val_acc: 0.8982\n",
            "Epoch 34/35\n",
            "3516/3515 [==============================] - 533s 152ms/step - loss: 0.2831 - acc: 0.9017 - val_loss: 0.3246 - val_acc: 0.9006\n",
            "Epoch 35/35\n",
            "3516/3515 [==============================] - 530s 151ms/step - loss: 0.2793 - acc: 0.9030 - val_loss: 0.3027 - val_acc: 0.9070\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZcWydmIVhZGr",
        "outputId": "44e966be-838a-4203-de7b-e546f3f734f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Test the model\n",
        "score = model.evaluate(X_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 10s 1ms/sample - loss: 0.3223 - acc: 0.9041\n",
            "Test loss: 0.32226717756986617\n",
            "Test accuracy: 0.9041\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UE3lF6EH1r_L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad989d6c-4591-4a42-9671-8104211b2ce5"
      },
      "source": [
        "# Save the trained weights in to .h5 format\n",
        "model.save_weights(\"DNST_model.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWpgfM8UwWHD",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niYS8nYgwZE2",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "1.   The most important parameters are : the value \"l\" which is number of layers in dense block and \"k\" which is denoted number of filters. \n",
        "\n",
        "2. I tried many values of l and k . Ultimately l=6 and k=35 proved to be effective. Also, the total number of parameters increase rapidly with these two parameters.\n",
        "\n",
        "3. Also, the process of early stopping as well as using decay function for learning rate also helped.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaAtdyUrwX3i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}