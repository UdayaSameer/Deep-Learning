{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kJntu98r93MF"
   },
   "outputs": [],
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "_k1Pe5J1BTzO",
    "outputId": "e488b298-e33a-4f48-c951-e28e41cc0658"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at HAR_Dataset\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('HAR_Dataset',force_remount=True)\n",
    "\n",
    "rootpath=\"HAR_Dataset/My Drive\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "seO4gQSx93MR"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "74H_mCHv93MW"
   },
   "outputs": [],
   "source": [
    "# Activities are the class labels\n",
    "# It is a 6 class classification\n",
    "ACTIVITIES = {\n",
    "    0: 'WALKING',\n",
    "    1: 'WALKING_UPSTAIRS',\n",
    "    2: 'WALKING_DOWNSTAIRS',\n",
    "    3: 'SITTING',\n",
    "    4: 'STANDING',\n",
    "    5: 'LAYING',\n",
    "}\n",
    "\n",
    "# Utility function to print the confusion matrix\n",
    "def confusion_matrix(Y_true, Y_pred):\n",
    "    Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_true, axis=1)])\n",
    "    Y_pred = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_pred, axis=1)])\n",
    "\n",
    "    return pd.crosstab(Y_true, Y_pred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c30wyUsa93Ma"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_2hwVxPo93Mb"
   },
   "outputs": [],
   "source": [
    "# Data directory\n",
    "DATADIR = 'UCI_HAR_Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EwuRRLTB93Mf"
   },
   "outputs": [],
   "source": [
    "# Raw data signals\n",
    "# Signals are from Accelerometer and Gyroscope\n",
    "# The signals are in x,y,z directions\n",
    "# Sensor signals are filtered to have only body acceleration\n",
    "# excluding the acceleration due to gravity\n",
    "# Triaxial acceleration from the accelerometer is total acceleration\n",
    "SIGNALS = [\n",
    "    \"body_acc_x\",\n",
    "    \"body_acc_y\",\n",
    "    \"body_acc_z\",\n",
    "    \"body_gyro_x\",\n",
    "    \"body_gyro_y\",\n",
    "    \"body_gyro_z\",\n",
    "    \"total_acc_x\",\n",
    "    \"total_acc_y\",\n",
    "    \"total_acc_z\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ClGJzrCM93Mi"
   },
   "outputs": [],
   "source": [
    "# Utility function to read the data from csv file\n",
    "def _read_csv(filename):\n",
    "    return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
    "\n",
    "# Utility function to load the load\n",
    "def load_signals(subset):\n",
    "    signals_data = []\n",
    "\n",
    "    for signal in SIGNALS:\n",
    "        filename = f'HAR_Dataset/My Drive/UCI_HAR_Dataset/{subset}/Inertial Signals/{signal}_{subset}.txt'\n",
    "        signals_data.append(\n",
    "            _read_csv(filename).as_matrix()\n",
    "        ) \n",
    "\n",
    "    # Transpose is used to change the dimensionality of the output,\n",
    "    # aggregating the signals by combination of sample/timestep.\n",
    "    # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n",
    "    return np.transpose(signals_data, (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gg6RQpD793Ml"
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_y(subset):\n",
    "    \"\"\"\n",
    "    The objective that we are trying to predict is a integer, from 1 to 6,\n",
    "    that represents a human activity. We return a binary representation of \n",
    "    every sample objective as a 6 bits vector using One Hot Encoding\n",
    "    (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
    "    \"\"\"\n",
    "    filename = f'HAR_Dataset/My Drive/UCI_HAR_Dataset/{subset}/y_{subset}.txt'\n",
    "    y = _read_csv(filename)[0]\n",
    "\n",
    "    return pd.get_dummies(y).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F-xSuyX_93Mo"
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Obtain the dataset from multiple files.\n",
    "    Returns: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    X_train, X_test = load_signals('train'), load_signals('test')\n",
    "    y_train, y_test = load_y('train'), load_y('test')\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sBvRoq3B93Ms"
   },
   "outputs": [],
   "source": [
    "# Importing tensorflow\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PxLtbFzi93Mw"
   },
   "outputs": [],
   "source": [
    "# Configuring a session\n",
    "session_conf = tf.ConfigProto(\n",
    "    intra_op_parallelism_threads=1,\n",
    "    inter_op_parallelism_threads=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Phe1JL1S93My",
    "outputId": "3819478c-95e9-48dd-ebf4-cf6f35df372d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import Keras\n",
    "from keras import backend as K\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ULWBFipT93M3"
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.core import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9yli5cu193M5"
   },
   "outputs": [],
   "source": [
    "# Initializing parameters\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "n_hidden = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fy4wcQ3j93M8"
   },
   "outputs": [],
   "source": [
    "# Utility function to count the number of classes\n",
    "def _count_classes(y):\n",
    "    return len(set([tuple(category) for category in y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "hSwnyVgh93NA",
    "outputId": "8b39c76c-7fad-4921-9eca-d3244fa046d5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# Loading the train and test data\n",
    "X_train, X_test, Y_train, Y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "G1ntNEyo93ND",
    "outputId": "2e6823d3-02c6-474c-ca6f-2bdbb45df0ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "9\n",
      "7352\n"
     ]
    }
   ],
   "source": [
    "timesteps = len(X_train[0])\n",
    "input_dim = len(X_train[0][0])\n",
    "n_classes = _count_classes(Y_train)\n",
    "\n",
    "print(timesteps)\n",
    "print(input_dim)\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HRrPiqBL93NI"
   },
   "source": [
    "# Assignment A) Hypertuning Num of Hidden Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6606
    },
    "colab_type": "code",
    "id": "-qDVUgiEwJkN",
    "outputId": "83b9ae71-f74b-48f9-c8b6-d50d498e01d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_11 (LSTM)               (None, 8)                 576       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 6)                 54        \n",
      "=================================================================\n",
      "Total params: 630\n",
      "Trainable params: 630\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 32s 4ms/step - loss: 1.5955 - acc: 0.3632 - val_loss: 1.4712 - val_acc: 0.4547\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 1.3659 - acc: 0.4550 - val_loss: 1.2986 - val_acc: 0.4625\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 1.2118 - acc: 0.4886 - val_loss: 1.1387 - val_acc: 0.5511\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 1.0938 - acc: 0.5291 - val_loss: 0.9670 - val_acc: 0.5986\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.9912 - acc: 0.5642 - val_loss: 0.9035 - val_acc: 0.5881\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.9305 - acc: 0.5809 - val_loss: 0.8377 - val_acc: 0.6200\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.8942 - acc: 0.5838 - val_loss: 0.8075 - val_acc: 0.6220\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.8780 - acc: 0.5903 - val_loss: 0.8696 - val_acc: 0.5874\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.8793 - acc: 0.5926 - val_loss: 0.8075 - val_acc: 0.6142\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.8699 - acc: 0.6002 - val_loss: 0.7906 - val_acc: 0.6088\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 29s 4ms/step - loss: 0.8410 - acc: 0.6042 - val_loss: 0.8077 - val_acc: 0.6071\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.8768 - acc: 0.5898 - val_loss: 0.8702 - val_acc: 0.5446\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.9041 - acc: 0.5709 - val_loss: 0.7830 - val_acc: 0.6118\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.8725 - acc: 0.5997 - val_loss: 0.7796 - val_acc: 0.6054\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.8794 - acc: 0.5838 - val_loss: 0.7690 - val_acc: 0.6169\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.8170 - acc: 0.6055 - val_loss: 0.7577 - val_acc: 0.6169\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.8236 - acc: 0.6107 - val_loss: 0.7586 - val_acc: 0.6166\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.7993 - acc: 0.6160 - val_loss: 0.7608 - val_acc: 0.6162\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.8132 - acc: 0.6137 - val_loss: 0.7347 - val_acc: 0.6196\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.7903 - acc: 0.6182 - val_loss: 0.7891 - val_acc: 0.6067\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.8168 - acc: 0.6096 - val_loss: 0.7569 - val_acc: 0.6152\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.8677 - acc: 0.5887 - val_loss: 0.8385 - val_acc: 0.5948\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.9627 - acc: 0.5664 - val_loss: 0.7984 - val_acc: 0.6040\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.8747 - acc: 0.5849 - val_loss: 0.7750 - val_acc: 0.6077\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.8360 - acc: 0.5979 - val_loss: 0.8559 - val_acc: 0.5881\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.8268 - acc: 0.6027 - val_loss: 0.7980 - val_acc: 0.5921\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.8024 - acc: 0.6113 - val_loss: 0.8121 - val_acc: 0.5752\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.8014 - acc: 0.6119 - val_loss: 0.7986 - val_acc: 0.5948\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 29s 4ms/step - loss: 0.7988 - acc: 0.6103 - val_loss: 0.7360 - val_acc: 0.6149\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.7801 - acc: 0.6163 - val_loss: 0.7498 - val_acc: 0.6067\n",
      "2947/2947 [==============================] - 1s 310us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_12 (LSTM)               (None, 16)                1664      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 6)                 102       \n",
      "=================================================================\n",
      "Total params: 1,766\n",
      "Trainable params: 1,766\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 33s 5ms/step - loss: 1.4487 - acc: 0.3957 - val_loss: 1.2815 - val_acc: 0.4398\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 31s 4ms/step - loss: 1.1780 - acc: 0.4678 - val_loss: 1.1603 - val_acc: 0.4255\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 32s 4ms/step - loss: 1.1275 - acc: 0.4739 - val_loss: 1.1512 - val_acc: 0.4455\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 32s 4ms/step - loss: 1.0714 - acc: 0.4884 - val_loss: 1.1699 - val_acc: 0.4544\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 32s 4ms/step - loss: 1.0013 - acc: 0.5394 - val_loss: 0.9824 - val_acc: 0.5748\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 32s 4ms/step - loss: 0.8838 - acc: 0.6175 - val_loss: 1.0248 - val_acc: 0.5633\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 32s 4ms/step - loss: 0.8260 - acc: 0.6488 - val_loss: 0.8740 - val_acc: 0.5918\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 31s 4ms/step - loss: 0.7483 - acc: 0.6778 - val_loss: 0.8256 - val_acc: 0.6128\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 31s 4ms/step - loss: 0.7194 - acc: 0.6812 - val_loss: 0.7590 - val_acc: 0.6447\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 31s 4ms/step - loss: 0.6657 - acc: 0.7163 - val_loss: 0.7619 - val_acc: 0.6630\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 31s 4ms/step - loss: 0.6537 - acc: 0.7145 - val_loss: 0.7595 - val_acc: 0.6783\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 31s 4ms/step - loss: 0.6750 - acc: 0.7187 - val_loss: 0.8579 - val_acc: 0.6868\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 31s 4ms/step - loss: 0.6330 - acc: 0.7295 - val_loss: 0.7055 - val_acc: 0.7058\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 31s 4ms/step - loss: 0.6436 - acc: 0.7382 - val_loss: 0.7825 - val_acc: 0.7109\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 32s 4ms/step - loss: 0.6338 - acc: 0.7533 - val_loss: 0.8779 - val_acc: 0.6946\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 31s 4ms/step - loss: 0.5908 - acc: 0.7652 - val_loss: 0.7379 - val_acc: 0.7333\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 31s 4ms/step - loss: 0.5812 - acc: 0.7595 - val_loss: 0.7077 - val_acc: 0.7289\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 32s 4ms/step - loss: 0.5537 - acc: 0.7677 - val_loss: 0.7166 - val_acc: 0.7401\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 31s 4ms/step - loss: 0.5465 - acc: 0.7777 - val_loss: 0.9780 - val_acc: 0.6994\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 31s 4ms/step - loss: 0.5127 - acc: 0.7926 - val_loss: 0.6583 - val_acc: 0.7791\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 32s 4ms/step - loss: 0.4995 - acc: 0.8292 - val_loss: 0.7334 - val_acc: 0.7689\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 31s 4ms/step - loss: 0.5634 - acc: 0.8281 - val_loss: 0.7062 - val_acc: 0.7845\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 31s 4ms/step - loss: 0.4479 - acc: 0.8602 - val_loss: 0.5615 - val_acc: 0.8235\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 32s 4ms/step - loss: 0.4057 - acc: 0.8803 - val_loss: 0.6156 - val_acc: 0.8103\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 31s 4ms/step - loss: 0.3659 - acc: 0.8920 - val_loss: 0.6306 - val_acc: 0.8130\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 32s 4ms/step - loss: 0.3439 - acc: 0.8984 - val_loss: 0.7598 - val_acc: 0.7940\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 32s 4ms/step - loss: 0.3343 - acc: 0.8981 - val_loss: 0.5796 - val_acc: 0.8215\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 31s 4ms/step - loss: 0.3665 - acc: 0.8915 - val_loss: 0.5604 - val_acc: 0.8334\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 31s 4ms/step - loss: 0.3177 - acc: 0.9087 - val_loss: 0.5977 - val_acc: 0.8422\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 31s 4ms/step - loss: 0.2947 - acc: 0.9087 - val_loss: 0.5064 - val_acc: 0.8487\n",
      "2947/2947 [==============================] - 1s 330us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_13 (LSTM)               (None, 32)                5376      \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 5,574\n",
      "Trainable params: 5,574\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 1.3670 - acc: 0.4244 - val_loss: 1.1861 - val_acc: 0.4812\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 1.0645 - acc: 0.5434 - val_loss: 0.9462 - val_acc: 0.6305\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 33s 5ms/step - loss: 0.8503 - acc: 0.6468 - val_loss: 1.1136 - val_acc: 0.5389\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 33s 5ms/step - loss: 0.7632 - acc: 0.6723 - val_loss: 0.7638 - val_acc: 0.6654\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 33s 5ms/step - loss: 0.6299 - acc: 0.7274 - val_loss: 0.7906 - val_acc: 0.7116\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 33s 4ms/step - loss: 0.5365 - acc: 0.7724 - val_loss: 0.7051 - val_acc: 0.7374\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 33s 5ms/step - loss: 0.4865 - acc: 0.7878 - val_loss: 0.5346 - val_acc: 0.7625\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 33s 5ms/step - loss: 0.4402 - acc: 0.7976 - val_loss: 0.5186 - val_acc: 0.7584\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 33s 4ms/step - loss: 0.4274 - acc: 0.8067 - val_loss: 0.5271 - val_acc: 0.7774\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.3714 - acc: 0.8312 - val_loss: 0.5221 - val_acc: 0.8032\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.3583 - acc: 0.8683 - val_loss: 0.5864 - val_acc: 0.8619\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 33s 4ms/step - loss: 0.3338 - acc: 0.8924 - val_loss: 0.4436 - val_acc: 0.8812\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.2621 - acc: 0.9230 - val_loss: 0.4836 - val_acc: 0.8809\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 33s 4ms/step - loss: 0.2387 - acc: 0.9263 - val_loss: 0.5588 - val_acc: 0.8578\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 33s 4ms/step - loss: 0.2641 - acc: 0.9165 - val_loss: 0.5041 - val_acc: 0.8683\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 33s 5ms/step - loss: 0.2405 - acc: 0.9189 - val_loss: 0.5069 - val_acc: 0.8829\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 33s 4ms/step - loss: 0.2195 - acc: 0.9309 - val_loss: 0.6124 - val_acc: 0.8826\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 33s 5ms/step - loss: 0.2089 - acc: 0.9295 - val_loss: 0.5830 - val_acc: 0.8677\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 33s 5ms/step - loss: 0.1861 - acc: 0.9312 - val_loss: 0.6641 - val_acc: 0.8677\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 33s 4ms/step - loss: 0.1760 - acc: 0.9387 - val_loss: 0.5216 - val_acc: 0.8955\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 33s 4ms/step - loss: 0.1834 - acc: 0.9357 - val_loss: 0.4442 - val_acc: 0.8860\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 33s 5ms/step - loss: 0.1873 - acc: 0.9357 - val_loss: 0.8061 - val_acc: 0.8585\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 33s 4ms/step - loss: 0.1811 - acc: 0.9376 - val_loss: 0.5185 - val_acc: 0.8955\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 33s 5ms/step - loss: 0.1882 - acc: 0.9391 - val_loss: 0.5882 - val_acc: 0.8816\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 33s 5ms/step - loss: 0.1621 - acc: 0.9395 - val_loss: 0.5554 - val_acc: 0.8945\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 33s 5ms/step - loss: 0.1590 - acc: 0.9433 - val_loss: 0.5083 - val_acc: 0.8979\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 33s 5ms/step - loss: 0.1739 - acc: 0.9410 - val_loss: 0.4687 - val_acc: 0.9101\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 33s 4ms/step - loss: 0.1762 - acc: 0.9448 - val_loss: 0.4780 - val_acc: 0.9033\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 33s 5ms/step - loss: 0.1490 - acc: 0.9431 - val_loss: 0.5573 - val_acc: 0.9070\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 33s 5ms/step - loss: 0.1680 - acc: 0.9430 - val_loss: 0.4472 - val_acc: 0.8992\n",
      "2947/2947 [==============================] - 1s 377us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_14 (LSTM)               (None, 64)                18944     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 19,334\n",
      "Trainable params: 19,334\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 41s 6ms/step - loss: 1.2066 - acc: 0.4890 - val_loss: 1.0286 - val_acc: 0.5358\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.9584 - acc: 0.5694 - val_loss: 0.8626 - val_acc: 0.6179\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.8011 - acc: 0.6730 - val_loss: 0.8045 - val_acc: 0.6617\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 0.6206 - acc: 0.7609 - val_loss: 0.6306 - val_acc: 0.7822\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.5015 - acc: 0.8267 - val_loss: 0.7192 - val_acc: 0.7652\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 0.4565 - acc: 0.8516 - val_loss: 0.5703 - val_acc: 0.7835\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 0.2895 - acc: 0.9008 - val_loss: 0.5809 - val_acc: 0.8381\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.2505 - acc: 0.9195 - val_loss: 0.3451 - val_acc: 0.8996\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 0.2373 - acc: 0.9298 - val_loss: 0.3195 - val_acc: 0.8928\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.2064 - acc: 0.9317 - val_loss: 0.2495 - val_acc: 0.9067\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 0.2192 - acc: 0.9305 - val_loss: 0.3046 - val_acc: 0.9080\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 0.2107 - acc: 0.9334 - val_loss: 0.2691 - val_acc: 0.8873\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.1739 - acc: 0.9397 - val_loss: 0.3186 - val_acc: 0.8982\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 0.1773 - acc: 0.9422 - val_loss: 0.2804 - val_acc: 0.9128\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.1875 - acc: 0.9411 - val_loss: 0.3220 - val_acc: 0.9043\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.1709 - acc: 0.9395 - val_loss: 0.3290 - val_acc: 0.9019\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.1560 - acc: 0.9433 - val_loss: 0.3034 - val_acc: 0.9046\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 0.1454 - acc: 0.9441 - val_loss: 0.2723 - val_acc: 0.9114\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 0.1786 - acc: 0.9342 - val_loss: 0.3256 - val_acc: 0.8945\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.1506 - acc: 0.9471 - val_loss: 0.2903 - val_acc: 0.9094\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 0.1713 - acc: 0.9436 - val_loss: 0.3093 - val_acc: 0.9080\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.1466 - acc: 0.9484 - val_loss: 0.5210 - val_acc: 0.9013\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.1567 - acc: 0.9470 - val_loss: 0.3900 - val_acc: 0.9091\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.1475 - acc: 0.9478 - val_loss: 0.3709 - val_acc: 0.9158\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.1833 - acc: 0.9436 - val_loss: 0.3424 - val_acc: 0.9172\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 0.1539 - acc: 0.9445 - val_loss: 0.2644 - val_acc: 0.9138\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.1573 - acc: 0.9472 - val_loss: 0.3868 - val_acc: 0.9138\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 0.2278 - acc: 0.9357 - val_loss: 0.3852 - val_acc: 0.8928\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.1552 - acc: 0.9474 - val_loss: 0.3414 - val_acc: 0.9128\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 0.1520 - acc: 0.9470 - val_loss: 0.3953 - val_acc: 0.9050\n",
      "2947/2947 [==============================] - 2s 526us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_15 (LSTM)               (None, 128)               70656     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 71,430\n",
      "Trainable params: 71,430\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 60s 8ms/step - loss: 1.2407 - acc: 0.4478 - val_loss: 1.0580 - val_acc: 0.5185\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 58s 8ms/step - loss: 0.9093 - acc: 0.5819 - val_loss: 1.1741 - val_acc: 0.4971\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 57s 8ms/step - loss: 0.7344 - acc: 0.6393 - val_loss: 0.7114 - val_acc: 0.6223\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 58s 8ms/step - loss: 0.6823 - acc: 0.6927 - val_loss: 0.6890 - val_acc: 0.7177\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 58s 8ms/step - loss: 0.4828 - acc: 0.8158 - val_loss: 0.4998 - val_acc: 0.8239\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 58s 8ms/step - loss: 0.3518 - acc: 0.8799 - val_loss: 0.7345 - val_acc: 0.8059\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 58s 8ms/step - loss: 0.2519 - acc: 0.9104 - val_loss: 0.3712 - val_acc: 0.8775\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 58s 8ms/step - loss: 0.2390 - acc: 0.9249 - val_loss: 0.2637 - val_acc: 0.8999\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 58s 8ms/step - loss: 0.1885 - acc: 0.9363 - val_loss: 0.3574 - val_acc: 0.8918\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 58s 8ms/step - loss: 0.1728 - acc: 0.9361 - val_loss: 0.4301 - val_acc: 0.8914\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 58s 8ms/step - loss: 0.1587 - acc: 0.9431 - val_loss: 0.3915 - val_acc: 0.8853\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 58s 8ms/step - loss: 0.1562 - acc: 0.9445 - val_loss: 0.3395 - val_acc: 0.8958\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 58s 8ms/step - loss: 0.1533 - acc: 0.9456 - val_loss: 0.4160 - val_acc: 0.8979\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 57s 8ms/step - loss: 0.1491 - acc: 0.9486 - val_loss: 0.5827 - val_acc: 0.8958\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 58s 8ms/step - loss: 0.1383 - acc: 0.9465 - val_loss: 0.3766 - val_acc: 0.9057\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 58s 8ms/step - loss: 0.1292 - acc: 0.9517 - val_loss: 0.4170 - val_acc: 0.9084\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 58s 8ms/step - loss: 0.1563 - acc: 0.9484 - val_loss: 0.3763 - val_acc: 0.9057\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 57s 8ms/step - loss: 0.1503 - acc: 0.9484 - val_loss: 0.4280 - val_acc: 0.9060\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 57s 8ms/step - loss: 0.1697 - acc: 0.9441 - val_loss: 0.2807 - val_acc: 0.9158\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 57s 8ms/step - loss: 0.1383 - acc: 0.9486 - val_loss: 0.4133 - val_acc: 0.9135\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 57s 8ms/step - loss: 0.1327 - acc: 0.9505 - val_loss: 0.4928 - val_acc: 0.8863\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 57s 8ms/step - loss: 0.1564 - acc: 0.9450 - val_loss: 0.4109 - val_acc: 0.9016\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 57s 8ms/step - loss: 0.1220 - acc: 0.9513 - val_loss: 0.5293 - val_acc: 0.8989\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 58s 8ms/step - loss: 0.1352 - acc: 0.9495 - val_loss: 0.4045 - val_acc: 0.9077\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 57s 8ms/step - loss: 0.1334 - acc: 0.9474 - val_loss: 0.4252 - val_acc: 0.9080\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 57s 8ms/step - loss: 0.1211 - acc: 0.9527 - val_loss: 0.4055 - val_acc: 0.9101\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 57s 8ms/step - loss: 0.1188 - acc: 0.9544 - val_loss: 0.5248 - val_acc: 0.8965\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 57s 8ms/step - loss: 0.1287 - acc: 0.9506 - val_loss: 0.5132 - val_acc: 0.9094\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 57s 8ms/step - loss: 0.1709 - acc: 0.9436 - val_loss: 0.4875 - val_acc: 0.9050\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 58s 8ms/step - loss: 0.1315 - acc: 0.9517 - val_loss: 0.8590 - val_acc: 0.8921\n",
      "2947/2947 [==============================] - 3s 943us/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-2acaa322236d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_Neurons_List\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of Hidden Neurons : %d, Accuracy : %d\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_Neurons_List\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_Neurons_Accuracy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: %d format: a number is required, not list"
     ]
    }
   ],
   "source": [
    "hidden_Neurons_List=[8,16,32,64,128]\n",
    "\n",
    "hidden_Neurons_Accuracy=[]\n",
    "\n",
    "for n_hidden in hidden_Neurons_List:\n",
    "  model=Sequential()\n",
    "  \n",
    "  model.add(LSTM(n_hidden,input_shape=(timesteps,input_dim)))\n",
    "  \n",
    "  model.add(Dropout(0.5))\n",
    "  \n",
    "  model.add(Dense(n_classes,activation='sigmoid'))\n",
    "  \n",
    "  model.summary()\n",
    "  \n",
    "  model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
    "  \n",
    "  model.fit(X_train,\n",
    "            Y_train,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_test, Y_test),\n",
    "            epochs=epochs)\n",
    "  \n",
    "  \n",
    "  \n",
    "  score = model.evaluate(X_test, Y_test)\n",
    "  \n",
    "  hidden_Neurons_Accuracy.append(score)\n",
    "  \n",
    "\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "Zwf0R2FZ5buU",
    "outputId": "26dab26a-7b8d-419e-f85a-9ac48853d3dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 16, 32, 64, 128]\n",
      "............\n",
      "[[0.7497640215505128, 0.6067186969799796], [0.5063654349498706, 0.8486596538853071], [0.44709858201512204, 0.8992195453003053], [0.3953069130833779, 0.9049881235154394], [0.8589523891387445, 0.8920936545841889]]\n"
     ]
    }
   ],
   "source": [
    "print(hidden_Neurons_List)\n",
    "print(\"............\")\n",
    "print(hidden_Neurons_Accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "qdKTjknb5pGu",
    "outputId": "342b35b4-25a5-4fc4-ae23-6ba205d8a075"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Hidden Neurons : 8, Accuracy : 0.606719\n",
      "Number of Hidden Neurons : 16, Accuracy : 0.848660\n",
      "Number of Hidden Neurons : 32, Accuracy : 0.899220\n",
      "Number of Hidden Neurons : 64, Accuracy : 0.904988\n",
      "Number of Hidden Neurons : 128, Accuracy : 0.892094\n"
     ]
    }
   ],
   "source": [
    "max_acc=0\n",
    "y=[]\n",
    "for i in range(len(hidden_Neurons_List)):\n",
    "  if hidden_Neurons_Accuracy[i][1]>max_acc:\n",
    "    max_acc=hidden_Neurons_Accuracy[i][1]\n",
    "    \n",
    "    best_n_hidden=hidden_Neurons_List[i]\n",
    "    \n",
    "  y.append(hidden_Neurons_Accuracy[i][1])\n",
    "  print(\"Number of Hidden Neurons : %d, Accuracy : %f\"%(hidden_Neurons_List[i],hidden_Neurons_Accuracy[i][1]))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "U3LhIzf_Ikaf",
    "outputId": "f670fe4c-12d3-4506-f3ca-0f7d2e22b02d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x = hidden_Neurons_List\n",
    "\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel('Number of hidden neurons')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bEDga0Al6K1K",
    "outputId": "cda52bf0-9a55-458b-bcc5-a8ebb365f4a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best value of number of hidden neurons is 64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"The best value of number of hidden neurons is %d\"%best_n_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WkEj5RHg6lgR"
   },
   "source": [
    "# Assignment B) Hypertuning Dropout Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Mhv5GC968Vj"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6038
    },
    "colab_type": "code",
    "id": "0l4lLReI6kC8",
    "outputId": "5c35bf17-8639-4090-e3de-502e60f6ca39"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0617 05:37:13.891773 140504591730560 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0617 05:37:13.895210 140504591730560 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0617 05:37:13.906264 140504591730560 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0617 05:37:14.170121 140504591730560 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0617 05:37:14.182163 140504591730560 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0617 05:37:14.214296 140504591730560 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0617 05:37:14.237277 140504591730560 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0617 05:37:14.389814 140504591730560 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 64)                18944     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 19,334\n",
      "Trainable params: 19,334\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 1.2624 - acc: 0.4302 - val_loss: 1.1988 - val_acc: 0.4618\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.9102 - acc: 0.5945 - val_loss: 0.9746 - val_acc: 0.6060\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.6894 - acc: 0.7201 - val_loss: 0.7941 - val_acc: 0.7282\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.6155 - acc: 0.7780 - val_loss: 0.7384 - val_acc: 0.7445\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.4409 - acc: 0.8474 - val_loss: 0.5518 - val_acc: 0.8029\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.3038 - acc: 0.8988 - val_loss: 0.4179 - val_acc: 0.8602\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.2346 - acc: 0.9174 - val_loss: 0.4541 - val_acc: 0.8375\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1999 - acc: 0.9291 - val_loss: 0.3426 - val_acc: 0.8836\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1781 - acc: 0.9366 - val_loss: 0.2973 - val_acc: 0.8965\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1564 - acc: 0.9369 - val_loss: 0.2566 - val_acc: 0.8948\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1501 - acc: 0.9438 - val_loss: 0.2811 - val_acc: 0.8992\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1606 - acc: 0.9419 - val_loss: 0.3061 - val_acc: 0.9043\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1453 - acc: 0.9429 - val_loss: 0.4754 - val_acc: 0.8897\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1542 - acc: 0.9453 - val_loss: 0.3720 - val_acc: 0.9013\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1489 - acc: 0.9426 - val_loss: 0.6335 - val_acc: 0.8561\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1412 - acc: 0.9468 - val_loss: 0.4738 - val_acc: 0.8921\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1386 - acc: 0.9464 - val_loss: 0.3489 - val_acc: 0.8992\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1273 - acc: 0.9476 - val_loss: 0.4205 - val_acc: 0.8972\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1273 - acc: 0.9505 - val_loss: 0.4278 - val_acc: 0.9013\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1309 - acc: 0.9502 - val_loss: 0.4471 - val_acc: 0.9097\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1280 - acc: 0.9502 - val_loss: 0.3473 - val_acc: 0.9111\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1233 - acc: 0.9504 - val_loss: 0.4305 - val_acc: 0.8809\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1221 - acc: 0.9508 - val_loss: 0.4307 - val_acc: 0.9101\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1172 - acc: 0.9538 - val_loss: 0.3566 - val_acc: 0.9087\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.1187 - acc: 0.9531 - val_loss: 0.4426 - val_acc: 0.8951\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1161 - acc: 0.9532 - val_loss: 0.4540 - val_acc: 0.9087\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1402 - acc: 0.9521 - val_loss: 0.4082 - val_acc: 0.8989\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1163 - acc: 0.9536 - val_loss: 0.5154 - val_acc: 0.9019\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1366 - acc: 0.9486 - val_loss: 0.3678 - val_acc: 0.8999\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1231 - acc: 0.9513 - val_loss: 0.2697 - val_acc: 0.9165\n",
      "2947/2947 [==============================] - 1s 482us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 64)                18944     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 19,334\n",
      "Trainable params: 19,334\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 1.1909 - acc: 0.4815 - val_loss: 0.9772 - val_acc: 0.5660\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.8578 - acc: 0.5934 - val_loss: 0.8286 - val_acc: 0.5813\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.7158 - acc: 0.6548 - val_loss: 0.7231 - val_acc: 0.6916\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.6563 - acc: 0.7359 - val_loss: 0.7523 - val_acc: 0.7346\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.5247 - acc: 0.8050 - val_loss: 0.6601 - val_acc: 0.7587\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.3771 - acc: 0.8769 - val_loss: 0.7364 - val_acc: 0.7964\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.2673 - acc: 0.9155 - val_loss: 0.4710 - val_acc: 0.8541\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.2216 - acc: 0.9222 - val_loss: 0.5025 - val_acc: 0.8554\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.2169 - acc: 0.9257 - val_loss: 0.6948 - val_acc: 0.8045\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1829 - acc: 0.9355 - val_loss: 0.4569 - val_acc: 0.8826\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1814 - acc: 0.9388 - val_loss: 0.2860 - val_acc: 0.8965\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1636 - acc: 0.9419 - val_loss: 0.3764 - val_acc: 0.8802\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.1737 - acc: 0.9400 - val_loss: 0.4120 - val_acc: 0.8884\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1586 - acc: 0.9449 - val_loss: 0.3797 - val_acc: 0.9036\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1422 - acc: 0.9459 - val_loss: 0.5289 - val_acc: 0.8809\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1587 - acc: 0.9467 - val_loss: 0.3084 - val_acc: 0.9111\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1336 - acc: 0.9504 - val_loss: 0.3853 - val_acc: 0.8972\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1427 - acc: 0.9490 - val_loss: 0.4564 - val_acc: 0.8795\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1423 - acc: 0.9489 - val_loss: 0.3197 - val_acc: 0.9009\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1414 - acc: 0.9493 - val_loss: 0.3731 - val_acc: 0.9057\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1332 - acc: 0.9474 - val_loss: 0.3390 - val_acc: 0.9101\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1370 - acc: 0.9480 - val_loss: 0.4441 - val_acc: 0.8945\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1298 - acc: 0.9514 - val_loss: 0.6037 - val_acc: 0.8734\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1684 - acc: 0.9387 - val_loss: 0.3698 - val_acc: 0.8924\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1460 - acc: 0.9450 - val_loss: 0.3180 - val_acc: 0.9135\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1249 - acc: 0.9497 - val_loss: 0.3963 - val_acc: 0.9016\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1340 - acc: 0.9501 - val_loss: 0.3784 - val_acc: 0.9002\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1403 - acc: 0.9471 - val_loss: 0.4159 - val_acc: 0.9063\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1313 - acc: 0.9532 - val_loss: 0.3031 - val_acc: 0.9114\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.1223 - acc: 0.9523 - val_loss: 0.5417 - val_acc: 0.8962\n",
      "2947/2947 [==============================] - 1s 490us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 64)                18944     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 19,334\n",
      "Trainable params: 19,334\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 1.2709 - acc: 0.4396 - val_loss: 1.1563 - val_acc: 0.4415\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.9985 - acc: 0.5336 - val_loss: 0.9617 - val_acc: 0.5779\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.8254 - acc: 0.6430 - val_loss: 0.8139 - val_acc: 0.6668\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.6177 - acc: 0.7556 - val_loss: 0.6214 - val_acc: 0.7954\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.4842 - acc: 0.8275 - val_loss: 0.4893 - val_acc: 0.8449\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.3198 - acc: 0.8998 - val_loss: 0.5319 - val_acc: 0.8310\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.2715 - acc: 0.9174 - val_loss: 0.4032 - val_acc: 0.8918\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.2208 - acc: 0.9251 - val_loss: 0.3455 - val_acc: 0.8901\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.2100 - acc: 0.9287 - val_loss: 0.3721 - val_acc: 0.8853\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1847 - acc: 0.9397 - val_loss: 0.4283 - val_acc: 0.8649\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1697 - acc: 0.9411 - val_loss: 0.3261 - val_acc: 0.9019\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1787 - acc: 0.9380 - val_loss: 0.4698 - val_acc: 0.8972\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1637 - acc: 0.9437 - val_loss: 0.3655 - val_acc: 0.9060\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1527 - acc: 0.9416 - val_loss: 0.3029 - val_acc: 0.8836\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1626 - acc: 0.9406 - val_loss: 0.5817 - val_acc: 0.8911\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1543 - acc: 0.9442 - val_loss: 0.2891 - val_acc: 0.9196\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1458 - acc: 0.9467 - val_loss: 0.2651 - val_acc: 0.9080\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1391 - acc: 0.9505 - val_loss: 0.3566 - val_acc: 0.9101\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1359 - acc: 0.9465 - val_loss: 0.3383 - val_acc: 0.9077\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1804 - acc: 0.9445 - val_loss: 0.5704 - val_acc: 0.8799\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.2048 - acc: 0.9441 - val_loss: 3.1658 - val_acc: 0.6946\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.2478 - acc: 0.9264 - val_loss: 0.4114 - val_acc: 0.9118\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1511 - acc: 0.9460 - val_loss: 0.3894 - val_acc: 0.9030\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.1383 - acc: 0.9516 - val_loss: 0.3022 - val_acc: 0.9118\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1342 - acc: 0.9508 - val_loss: 0.3223 - val_acc: 0.9145\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1315 - acc: 0.9509 - val_loss: 0.2886 - val_acc: 0.9206\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1282 - acc: 0.9483 - val_loss: 0.2960 - val_acc: 0.9172\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1411 - acc: 0.9489 - val_loss: 0.4103 - val_acc: 0.9043\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1463 - acc: 0.9463 - val_loss: 0.4754 - val_acc: 0.9118\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1289 - acc: 0.9513 - val_loss: 0.4099 - val_acc: 0.9084\n",
      "2947/2947 [==============================] - 1s 494us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0617 06:32:15.475296 140504591730560 nn_ops.py:4224] Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 64)                18944     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 19,334\n",
      "Trainable params: 19,334\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 1.3130 - acc: 0.4402 - val_loss: 1.2271 - val_acc: 0.4048\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 1.0343 - acc: 0.5465 - val_loss: 1.0059 - val_acc: 0.5969\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.8394 - acc: 0.6143 - val_loss: 0.8697 - val_acc: 0.6033\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.7459 - acc: 0.6659 - val_loss: 0.7944 - val_acc: 0.6614\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.7040 - acc: 0.7108 - val_loss: 0.8154 - val_acc: 0.6759\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.5503 - acc: 0.8098 - val_loss: 0.6159 - val_acc: 0.8151\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.4508 - acc: 0.8615 - val_loss: 0.5649 - val_acc: 0.8439\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.3289 - acc: 0.9038 - val_loss: 0.6414 - val_acc: 0.8527\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.2942 - acc: 0.9064 - val_loss: 0.5069 - val_acc: 0.8639\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.2701 - acc: 0.9168 - val_loss: 0.4653 - val_acc: 0.8782\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.2548 - acc: 0.9246 - val_loss: 0.4721 - val_acc: 0.8782\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.2221 - acc: 0.9291 - val_loss: 0.3499 - val_acc: 0.9050\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.2188 - acc: 0.9334 - val_loss: 0.4939 - val_acc: 0.8887\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1972 - acc: 0.9369 - val_loss: 0.4614 - val_acc: 0.9108\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1874 - acc: 0.9365 - val_loss: 0.4031 - val_acc: 0.9097\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1929 - acc: 0.9374 - val_loss: 0.3830 - val_acc: 0.9175\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.2103 - acc: 0.9339 - val_loss: 0.3160 - val_acc: 0.9162\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1873 - acc: 0.9422 - val_loss: 0.4866 - val_acc: 0.9046\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.2091 - acc: 0.9384 - val_loss: 0.5327 - val_acc: 0.8931\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.1869 - acc: 0.9422 - val_loss: 0.3995 - val_acc: 0.9023\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: nan - acc: 0.9393 - val_loss: 0.3918 - val_acc: 0.9026\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: nan - acc: 0.9463 - val_loss: 0.3753 - val_acc: 0.9179\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: nan - acc: 0.9404 - val_loss: 0.3419 - val_acc: 0.9213\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: nan - acc: 0.9433 - val_loss: 0.4082 - val_acc: 0.9135\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: nan - acc: 0.9444 - val_loss: 0.5373 - val_acc: 0.8968\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 43s 6ms/step - loss: nan - acc: 0.9412 - val_loss: 0.4829 - val_acc: 0.9118\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: nan - acc: 0.9444 - val_loss: 0.5000 - val_acc: 0.9053\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: nan - acc: 0.9412 - val_loss: 0.4524 - val_acc: 0.9050\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: nan - acc: 0.9459 - val_loss: 0.6087 - val_acc: 0.9026\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: nan - acc: 0.9425 - val_loss: 0.5606 - val_acc: 0.8765\n",
      "2947/2947 [==============================] - 1s 487us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0617 06:51:05.217503 140504591730560 nn_ops.py:4224] Large dropout rate: 0.9 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 64)                18944     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 19,334\n",
      "Trainable params: 19,334\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 1.4367 - acc: 0.3902 - val_loss: 1.2651 - val_acc: 0.4340\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 1.2442 - acc: 0.4649 - val_loss: 1.0755 - val_acc: 0.5199\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 1.1148 - acc: 0.5132 - val_loss: 1.3731 - val_acc: 0.3940\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 1.0714 - acc: 0.5355 - val_loss: 1.0820 - val_acc: 0.5473\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.9871 - acc: 0.5768 - val_loss: 0.8221 - val_acc: 0.5921\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.9340 - acc: 0.5944 - val_loss: 1.1360 - val_acc: 0.5789\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.8972 - acc: 0.6145 - val_loss: 0.8657 - val_acc: 0.6074\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 1.0225 - acc: 0.5970 - val_loss: 0.9634 - val_acc: 0.5969\n",
      "Epoch 9/30\n",
      "4624/7352 [=================>............] - ETA: 12s - loss: 0.8600 - acc: 0.6122Buffered data was truncated after reaching the output size limit."
     ]
    }
   ],
   "source": [
    "dropout_List=[0.1,0.3,0.5,0.7,0.9]\n",
    "\n",
    "dropout_Accuracy=[]\n",
    "\n",
    "for dropout in dropout_List:\n",
    "  model=Sequential()\n",
    "  \n",
    "  model.add(LSTM(best_n_hidden,input_shape=(timesteps,input_dim)))\n",
    "  \n",
    "  model.add(Dropout(dropout))\n",
    "  \n",
    "  model.add(Dense(n_classes,activation='sigmoid'))\n",
    "  \n",
    "  model.summary()\n",
    "  \n",
    "  model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
    "  \n",
    "  model.fit(X_train,\n",
    "            Y_train,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_test, Y_test),\n",
    "            epochs=epochs)\n",
    "  \n",
    "  \n",
    "  \n",
    "  score = model.evaluate(X_test, Y_test)\n",
    "  \n",
    "  dropout_Accuracy.append(score)\n",
    "  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "R94-13sSQGYv",
    "outputId": "61213ce9-0ea5-4b5a-8ae4-55cb8d0b89a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1, 0.3, 0.5, 0.7, 0.9]\n",
      "............\n",
      "[[0.26970847920445745, 0.9165252799457075], [0.5416957346667051, 0.8961655921275874], [0.40993680355903883, 0.9083814048184594], [0.560796002992605, 0.8764845605700713], [1.5101637662438, 0.4872751951136749]]\n"
     ]
    }
   ],
   "source": [
    "print(dropout_List)\n",
    "print(\"............\")\n",
    "print(dropout_Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "vhCCYUS669ut",
    "outputId": "ae9a1741-bc39-4224-826b-ab2cd8514a31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1, 0.3, 0.5, 0.7, 0.9]\n",
      "............\n",
      "[[0.26970847920445745, 0.9165252799457075], [0.5416957346667051, 0.8961655921275874], [0.40993680355903883, 0.9083814048184594], [0.560796002992605, 0.8764845605700713], [1.5101637662438, 0.4872751951136749]]\n",
      "Number of Hidden Neurons : 0, Accuracy : 0.916525\n",
      "Number of Hidden Neurons : 0, Accuracy : 0.896166\n",
      "Number of Hidden Neurons : 0, Accuracy : 0.908381\n",
      "Number of Hidden Neurons : 0, Accuracy : 0.876485\n",
      "Number of Hidden Neurons : 0, Accuracy : 0.487275\n",
      "..........\n",
      "\n",
      "The best droput value is : 0.100000\n"
     ]
    }
   ],
   "source": [
    "print(dropout_List)\n",
    "print(\"............\")\n",
    "print(dropout_Accuracy)\n",
    "\n",
    "\n",
    "\n",
    "max_acc=0\n",
    "best_dropout=0\n",
    "y=[]\n",
    "for i in range(len(dropout_List)):\n",
    "  if dropout_Accuracy[i][1]>max_acc:\n",
    "    max_acc=dropout_Accuracy[i][1]\n",
    "    best_dropout=dropout_List[i]\n",
    "  y.append(dropout_Accuracy[i][1])  \n",
    "  print(\"Number of Hidden Neurons : %d, Accuracy : %f\"%(dropout_List[i],dropout_Accuracy[i][1]))\n",
    "\n",
    "print(\"..........\\n\")\n",
    "print(\"The best droput value is : %f\"%best_dropout)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "7tctCM8tKTS0",
    "outputId": "6064a88d-7e87-41f0-f370-a754e3da4fac"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x = dropout_List\n",
    "\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel('Dropout Rate')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iJb1xitZTik_"
   },
   "source": [
    "# Assignment C) 2 Layer LSTM and Higher dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3AYQVLBYRpI4"
   },
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "\n",
    "from keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ofVhiSZ1-hYG"
   },
   "outputs": [],
   "source": [
    "epochs = 75\n",
    "kernel_size = 1 \n",
    "pool_size = 2\n",
    "\n",
    "dropout_rate = 0.1\n",
    "\n",
    "f_act = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "OO5st4dBSXEl",
    "outputId": "cb830059-7d37-4d34-dc00-0aab4aab8668"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/75\n",
      "7352/7352 [==============================] - 76s 10ms/step - loss: 0.6044 - acc: 0.7349 - val_loss: 0.4009 - val_acc: 0.8412\n",
      "Epoch 2/75\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.2458 - acc: 0.9142 - val_loss: 0.2614 - val_acc: 0.9040\n",
      "Epoch 3/75\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.1767 - acc: 0.9347 - val_loss: 0.2164 - val_acc: 0.9155\n",
      "Epoch 4/75\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.1652 - acc: 0.9395 - val_loss: 0.2499 - val_acc: 0.9189\n",
      "Epoch 5/75\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.1463 - acc: 0.9402 - val_loss: 0.2175 - val_acc: 0.9101\n",
      "Epoch 6/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.1405 - acc: 0.9475 - val_loss: 0.1765 - val_acc: 0.9277\n",
      "Epoch 7/75\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.1327 - acc: 0.9470 - val_loss: 0.2068 - val_acc: 0.9281\n",
      "Epoch 8/75\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.1376 - acc: 0.9440 - val_loss: 0.2475 - val_acc: 0.9063\n",
      "Epoch 9/75\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.1231 - acc: 0.9508 - val_loss: 0.1870 - val_acc: 0.9298\n",
      "Epoch 10/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.1273 - acc: 0.9482 - val_loss: 0.2394 - val_acc: 0.9264\n",
      "Epoch 11/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.1260 - acc: 0.9490 - val_loss: 0.2158 - val_acc: 0.9328\n",
      "Epoch 12/75\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.1253 - acc: 0.9478 - val_loss: 0.2385 - val_acc: 0.9145\n",
      "Epoch 13/75\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.1237 - acc: 0.9468 - val_loss: 0.1687 - val_acc: 0.9230\n",
      "Epoch 14/75\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.1246 - acc: 0.9487 - val_loss: 0.2370 - val_acc: 0.9152\n",
      "Epoch 15/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.1303 - acc: 0.9474 - val_loss: 0.1784 - val_acc: 0.9287\n",
      "Epoch 16/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.1215 - acc: 0.9495 - val_loss: 0.1804 - val_acc: 0.9399\n",
      "Epoch 17/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.1140 - acc: 0.9543 - val_loss: 0.2006 - val_acc: 0.9264\n",
      "Epoch 18/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.1081 - acc: 0.9540 - val_loss: 0.2364 - val_acc: 0.9328\n",
      "Epoch 19/75\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.1114 - acc: 0.9559 - val_loss: 0.2495 - val_acc: 0.9196\n",
      "Epoch 20/75\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.1125 - acc: 0.9533 - val_loss: 0.1758 - val_acc: 0.9277\n",
      "Epoch 21/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.1108 - acc: 0.9540 - val_loss: 0.2155 - val_acc: 0.9257\n",
      "Epoch 22/75\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.1160 - acc: 0.9520 - val_loss: 0.1973 - val_acc: 0.9260\n",
      "Epoch 23/75\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.1189 - acc: 0.9494 - val_loss: 0.2309 - val_acc: 0.9216\n",
      "Epoch 24/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.1029 - acc: 0.9581 - val_loss: 0.1899 - val_acc: 0.9287\n",
      "Epoch 25/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.1085 - acc: 0.9561 - val_loss: 0.2171 - val_acc: 0.9352\n",
      "Epoch 26/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.1045 - acc: 0.9584 - val_loss: 0.1599 - val_acc: 0.9437\n",
      "Epoch 27/75\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.1038 - acc: 0.9584 - val_loss: 0.2012 - val_acc: 0.9338\n",
      "Epoch 28/75\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.0988 - acc: 0.9607 - val_loss: 0.1979 - val_acc: 0.9393\n",
      "Epoch 29/75\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.0920 - acc: 0.9634 - val_loss: 0.2130 - val_acc: 0.9376\n",
      "Epoch 30/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.0936 - acc: 0.9627 - val_loss: 0.1882 - val_acc: 0.9410\n",
      "Epoch 31/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.0985 - acc: 0.9601 - val_loss: 0.2218 - val_acc: 0.9515\n",
      "Epoch 32/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.0959 - acc: 0.9619 - val_loss: 0.1977 - val_acc: 0.9315\n",
      "Epoch 33/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.0933 - acc: 0.9642 - val_loss: 0.2235 - val_acc: 0.9427\n",
      "Epoch 34/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.0889 - acc: 0.9637 - val_loss: 0.1731 - val_acc: 0.9467\n",
      "Epoch 35/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.0952 - acc: 0.9635 - val_loss: 0.2437 - val_acc: 0.9389\n",
      "Epoch 36/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.0887 - acc: 0.9642 - val_loss: 0.2136 - val_acc: 0.9525\n",
      "Epoch 37/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.0908 - acc: 0.9645 - val_loss: 0.2294 - val_acc: 0.9332\n",
      "Epoch 38/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.0866 - acc: 0.9664 - val_loss: 0.2182 - val_acc: 0.9423\n",
      "Epoch 39/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.0799 - acc: 0.9682 - val_loss: 0.1753 - val_acc: 0.9501\n",
      "Epoch 40/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.0926 - acc: 0.9656 - val_loss: 0.1889 - val_acc: 0.9332\n",
      "Epoch 41/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.0804 - acc: 0.9695 - val_loss: 0.1853 - val_acc: 0.9464\n",
      "Epoch 42/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.0847 - acc: 0.9679 - val_loss: 0.2067 - val_acc: 0.9444\n",
      "Epoch 43/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.0749 - acc: 0.9721 - val_loss: 0.1476 - val_acc: 0.9494\n",
      "Epoch 44/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.0834 - acc: 0.9668 - val_loss: 0.2414 - val_acc: 0.9328\n",
      "Epoch 45/75\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.0744 - acc: 0.9694 - val_loss: 0.2256 - val_acc: 0.9386\n",
      "Epoch 46/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.0689 - acc: 0.9708 - val_loss: 0.2155 - val_acc: 0.9365\n",
      "Epoch 47/75\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.0731 - acc: 0.9723 - val_loss: 0.1623 - val_acc: 0.9525\n",
      "Epoch 48/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.0761 - acc: 0.9690 - val_loss: 0.2077 - val_acc: 0.9444\n",
      "Epoch 49/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.0659 - acc: 0.9737 - val_loss: 0.1698 - val_acc: 0.9505\n",
      "Epoch 50/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.0748 - acc: 0.9733 - val_loss: 0.2536 - val_acc: 0.9433\n",
      "Epoch 51/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.0634 - acc: 0.9739 - val_loss: 0.1756 - val_acc: 0.9555\n",
      "Epoch 52/75\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.0671 - acc: 0.9728 - val_loss: 0.1926 - val_acc: 0.9508\n",
      "Epoch 53/75\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.0681 - acc: 0.9736 - val_loss: 0.1535 - val_acc: 0.9552\n",
      "Epoch 54/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.0602 - acc: 0.9765 - val_loss: 0.2422 - val_acc: 0.9396\n",
      "Epoch 55/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.0579 - acc: 0.9770 - val_loss: 0.2728 - val_acc: 0.9454\n",
      "Epoch 56/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.0590 - acc: 0.9755 - val_loss: 0.2370 - val_acc: 0.9379\n",
      "Epoch 57/75\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.0600 - acc: 0.9765 - val_loss: 0.1935 - val_acc: 0.9511\n",
      "Epoch 58/75\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.0619 - acc: 0.9758 - val_loss: 0.1887 - val_acc: 0.9508\n",
      "Epoch 59/75\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.0594 - acc: 0.9777 - val_loss: 0.2077 - val_acc: 0.9437\n",
      "Epoch 60/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.0530 - acc: 0.9782 - val_loss: 0.2571 - val_acc: 0.9416\n",
      "Epoch 61/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.0599 - acc: 0.9765 - val_loss: 0.1669 - val_acc: 0.9532\n",
      "Epoch 62/75\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.0532 - acc: 0.9793 - val_loss: 0.2154 - val_acc: 0.9532\n",
      "Epoch 63/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.0473 - acc: 0.9803 - val_loss: 0.1688 - val_acc: 0.9562\n",
      "Epoch 64/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.0585 - acc: 0.9762 - val_loss: 0.1869 - val_acc: 0.9484\n",
      "Epoch 65/75\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.0506 - acc: 0.9804 - val_loss: 0.1693 - val_acc: 0.9579\n",
      "Epoch 66/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.0502 - acc: 0.9803 - val_loss: 0.1838 - val_acc: 0.9603\n",
      "Epoch 67/75\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.0483 - acc: 0.9814 - val_loss: 0.2321 - val_acc: 0.9474\n",
      "Epoch 68/75\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.0486 - acc: 0.9801 - val_loss: 0.2033 - val_acc: 0.9569\n",
      "Epoch 69/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.0506 - acc: 0.9804 - val_loss: 0.1534 - val_acc: 0.9640\n",
      "Epoch 70/75\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.0427 - acc: 0.9835 - val_loss: 0.1772 - val_acc: 0.9637\n",
      "Epoch 71/75\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.0418 - acc: 0.9823 - val_loss: 0.1770 - val_acc: 0.9620\n",
      "Epoch 72/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.0465 - acc: 0.9805 - val_loss: 0.1496 - val_acc: 0.9698\n",
      "Epoch 73/75\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.0388 - acc: 0.9835 - val_loss: 0.1730 - val_acc: 0.9566\n",
      "Epoch 74/75\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.0427 - acc: 0.9833 - val_loss: 0.1377 - val_acc: 0.9671\n",
      "Epoch 75/75\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.0455 - acc: 0.9811 - val_loss: 0.1823 - val_acc: 0.9586\n",
      "2947/2947 [==============================] - 4s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(512, (kernel_size), input_shape=(X_train.shape[1],X_train.shape[2]), activation=f_act, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=(pool_size)))\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "model.add(Conv1D(64, (kernel_size), activation=f_act, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=(pool_size)))\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "\n",
    "model.add(Conv1D(32, (kernel_size), activation=f_act, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=(pool_size)))\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(LSTM(64))\n",
    "\n",
    "\n",
    "model.add(Dense(n_classes,activation='sigmoid'))\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train,\n",
    "         Y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f6Ifn27GTg4a"
   },
   "outputs": [],
   "source": [
    "#model=Sequential()\n",
    "\n",
    "#model.add(LSTM(200,return_sequences=True,input_shape=(timesteps,input_dim)))\n",
    "\n",
    "#model.add(LSTM(100))\n",
    "\n",
    "#model.add(Dropout(0.8))\n",
    "\n",
    "#model.add(Dense(n_classes,activation='sigmoid'))\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "#model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
    "\n",
    "#model.fit(X_train,\n",
    "#         Y_train,\n",
    "#          batch_size=batch_size,\n",
    "#          validation_data=(X_test, Y_test),\n",
    "#          epochs=epochs)\n",
    "\n",
    "\n",
    "\n",
    "#score = model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dNMC0RAVjHfN"
   },
   "outputs": [],
   "source": [
    "#print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0wVhu58SZOOS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "A3moRyRQ93NZ",
    "outputId": "9c3d7da5-ad05-4037-b3b4-978b24f27da8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred                LAYING  SITTING  ...  WALKING_DOWNSTAIRS  WALKING_UPSTAIRS\n",
      "True                                 ...                                      \n",
      "LAYING                 537        0  ...                   0                 0\n",
      "SITTING                  5      433  ...                   0                 3\n",
      "STANDING                 0       27  ...                   0                 0\n",
      "WALKING                  0        0  ...                   3                 0\n",
      "WALKING_DOWNSTAIRS       0        0  ...                 415                 2\n",
      "WALKING_UPSTAIRS         0        0  ...                   0               442\n",
      "\n",
      "[6 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "print(confusion_matrix(Y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Rr5U3A4D93Nd",
    "outputId": "372b52b9-249d-4daa-90fd-47ec6efed92a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2947/2947 [==============================] - 4s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fGOEUGdZGlTl",
    "outputId": "46246014-35f9-421d-c48a-b0f2a64d1271"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1823121076857176, 0.9637048147261827]\n"
     ]
    }
   ],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ULPHnYN2HSDO"
   },
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "2pZR7OwXGGqC",
    "outputId": "205a1d65-178f-4307-a722-00f34e0fdf54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------------------------+-------------------------+--------------------+\n",
      "| Excercise |                Task               |         Remarks         |      Accuracy      |\n",
      "+-----------+-----------------------------------+-------------------------+--------------------+\n",
      "|     A     |     Hypertuning Hidden Neurons    |  Best value found at 64 |      0.904988      |\n",
      "|     B     |     Hypertuning Dropout value     | Best value found at 0.1 |      0.916525      |\n",
      "|     C     | To improve accuracy to beyond 96% |     CNN + LSTM used     | 0.9637048147261827 |\n",
      "+-----------+-----------------------------------+-------------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "x = PrettyTable()\n",
    "x.field_names = [\"Excercise\",\"Task\",\"Remarks\",\"Accuracy\"]\n",
    "\n",
    "\n",
    "x.add_row([\"A\",\"Hypertuning Hidden Neurons\",\"Best value found at 64\",0.904988])\n",
    "x.add_row([\"B\",\"Hypertuning Dropout value\",\"Best value found at 0.1\",0.916525])\n",
    "x.add_row([\"C\",\"To improve accuracy to beyond 96%\",\"CNN + LSTM used\",0.9637048147261827])\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RQoBCThrIb8F"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HAR_LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
